from flask import Flask, render_template, request, jsonify, send_file
import asyncio
import json
import datetime
import os
import random
import time
from urllib.parse import urlparse
import threading
from concurrent.futures import ThreadPoolExecutor

# æ™ºæ…§ä¾è³´æª¢æŸ¥
def check_dependencies():
    """æª¢æŸ¥ä¸¦è‡ªå‹•å®‰è£ç¼ºå¤±çš„ä¾è³´"""
    missing_deps = []
    
    try:
        from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
    except ImportError:
        missing_deps.append("playwright")
    
    try:
        import flask
    except ImportError:
        missing_deps.append("flask")
    
    if missing_deps:
        print("âŒ æª¢æ¸¬åˆ°ç¼ºå¤±çš„ä¾è³´å¥—ä»¶:")
        for dep in missing_deps:
            print(f"   - {dep}")
        
        print("\nè«‹åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤å®‰è£ä¾è³´:")
        print("pip install flask playwright")
        print("playwright install chromium")
        return False
    
    return True

# æª¢æŸ¥ä¾è³´
if not check_dependencies():
    exit(1)

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

app = Flask(__name__)

# ååµæ¸¬ç­–ç•¥é…ç½®
ANTI_DETECTION_CONFIG = {
    "user_agents": [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59"
    ],
    "viewport_sizes": [
        {"width": 1920, "height": 1080},
        {"width": 1366, "height": 768},
        {"width": 1440, "height": 900},
        {"width": 1536, "height": 864}
    ],
    "delays": {
        "page_load": (2, 5),
        "between_actions": (1, 3),
        "between_requests": (3, 8)
    }
}

async def extract_caption_from_meta(page) -> tuple[str, str]:
    """å¾ meta æ¨™ç±¤ä¸­æå–æ–‡æ¡ˆ"""
    try:
        # å˜—è©¦å¾ og:description meta æ¨™ç±¤ç²å–
        og_description = await page.get_attribute('meta[property="og:description"]', 'content')
        if og_description and len(og_description) > 50:
            match = re.search(r':\s*"([^"]+)"', og_description)
            if match:
                return match.group(1).strip(), "meta_og_description"
            cleaned = re.sub(r'^[^:]+:\s*', '', og_description)
            return cleaned.strip(), "meta_og_description"
        
        # å˜—è©¦å¾ description meta æ¨™ç±¤ç²å–
        description = await page.get_attribute('meta[name="description"]', 'content')
        if description and len(description) > 50:
            match = re.search(r':\s*"([^"]+)"', description)
            if match:
                return match.group(1).strip(), "meta_description"
            cleaned = re.sub(r'^[^:]+:\s*', '', description)
            return cleaned.strip(), "meta_description"
            
    except Exception as e:
        print(f"Meta æ¨™ç±¤æŠ“å–å¤±æ•—: {e}")
    
    return "", "failed"

async def get_single_post_caption(url: str, browser_context=None) -> dict:
    """æŠ“å–å–®å€‹Instagramè²¼æ–‡çš„æ–‡æ¡ˆï¼ŒåŒ…å«ååµæ¸¬ç­–ç•¥"""
    result = {
        "url": url,
        "timestamp": datetime.datetime.now().isoformat(),
        "caption": "",
        "success": False,
        "method": "",
        "length": 0,
        "error": None
    }
    
    try:
        # ååµæ¸¬ç­–ç•¥ï¼šéš¨æ©Ÿé¸æ“‡é…ç½®
        user_agent = random.choice(ANTI_DETECTION_CONFIG["user_agents"])
        viewport = random.choice(ANTI_DETECTION_CONFIG["viewport_sizes"])
        
        # å¦‚æœæ²’æœ‰æä¾›browser contextï¼Œå‰µå»ºæ–°çš„
        if browser_context is None:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
                )
                context = await browser.new_context(
                    user_agent=user_agent,
                    viewport=viewport
                )
                page = await context.new_page()
                try:
                    result = await _extract_caption_from_page(page, url, result)
                finally:
                    await context.close()
                    await browser.close()
        else:
            page = await browser_context.new_page()
            try:
                result = await _extract_caption_from_page(page, url, result)
            finally:
                await page.close()
                
    except Exception as e:
        result["error"] = f"æœªé æœŸçš„éŒ¯èª¤: {str(e)}"
    
    return result

async def _extract_caption_from_page(page, url: str, result: dict) -> dict:
    """å¾é é¢æå–æ–‡æ¡ˆçš„æ ¸å¿ƒé‚è¼¯"""
    try:
        # ååµæ¸¬ç­–ç•¥ï¼šæ·»åŠ éš¨æ©Ÿå»¶é²
        await asyncio.sleep(random.uniform(*ANTI_DETECTION_CONFIG["delays"]["page_load"]))
        
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        await asyncio.sleep(random.uniform(*ANTI_DETECTION_CONFIG["delays"]["page_load"]))

        # é—œé–‰å½ˆçª—
        await page.keyboard.press('Escape')
        await asyncio.sleep(random.uniform(*ANTI_DETECTION_CONFIG["delays"]["between_actions"]))

        caption = ""
        method = ""
        
        # å˜—è©¦å¾é é¢å…ƒç´ æŠ“å–æ–‡æ¡ˆ
        caption_selectors = [
            'article div[data-testid="post-shared-text"] span',
            'article span[dir="auto"]',
            'div[data-testid="post-shared-text"]',
            'span[dir="auto"]',
            'div[role="button"] span',
        ]
        
        for selector in caption_selectors:
            try:
                elements = page.locator(selector)
                count = await elements.count()
                
                if count > 0:
                    for i in range(min(count, 10)):  # é™åˆ¶æª¢æŸ¥æ•¸é‡
                        try:
                            text = await elements.nth(i).inner_text()
                            text = text.strip()
                            
                            if (len(text) > 20 and 
                                "å¾ˆæŠ±æ­‰" not in text and 
                                "æ’­æ”¾æ­¤å½±ç‰‡æ™‚ç™¼ç”Ÿå•é¡Œ" not in text and
                                "Instagram" not in text and
                                "ç™»å…¥" not in text):
                                
                                if len(text) > len(caption):
                                    caption = text
                                    method = f"page_element_{selector}"
                        except:
                            continue
                            
            except:
                continue
        
        # å¦‚æœé é¢æŠ“å–çµæœä¸ç†æƒ³ï¼Œå˜—è©¦metaæ¨™ç±¤
        if not caption or len(caption) < 30:
            meta_caption, meta_method = await extract_caption_from_meta(page)
            if meta_caption and len(meta_caption) > len(caption):
                caption = meta_caption
                method = meta_method
        
        if caption and len(caption) > 10:
            result["caption"] = caption
            result["success"] = True
            result["method"] = method
            result["length"] = len(caption)
        else:
            result["error"] = "ç„¡æ³•æ‰¾åˆ°æ–‡æ¡ˆå…§å®¹"
            
    except PlaywrightTimeoutError:
        result["error"] = "é é¢è¼‰å…¥è¶…æ™‚"
    except Exception as e:
        result["error"] = f"æŠ“å–éç¨‹éŒ¯èª¤: {str(e)}"
    
    return result

async def batch_extract_captions(urls: list) -> list:
    """æ‰¹é‡æŠ“å–Instagramæ–‡æ¡ˆï¼Œä½¿ç”¨ååµæ¸¬ç­–ç•¥"""
    results = []
    
    # é™åˆ¶æœ€å¤š5å€‹URL
    urls = urls[:5]
    
    async with async_playwright() as p:
        # ååµæ¸¬ç­–ç•¥ï¼šä½¿ç”¨éš¨æ©Ÿé…ç½®
        user_agent = random.choice(ANTI_DETECTION_CONFIG["user_agents"])
        viewport = random.choice(ANTI_DETECTION_CONFIG["viewport_sizes"])
        
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox', 
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        
        context = await browser.new_context(
            user_agent=user_agent,
            viewport=viewport
        )
        
        try:
            for i, url in enumerate(urls):
                if i > 0:
                    # ååµæ¸¬ç­–ç•¥ï¼šè«‹æ±‚é–“éš¨æ©Ÿå»¶é²
                    delay = random.uniform(*ANTI_DETECTION_CONFIG["delays"]["between_requests"])
                    print(f"ç­‰å¾… {delay:.1f} ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹URL...")
                    await asyncio.sleep(delay)
                
                print(f"è™•ç†ç¬¬ {i+1}/{len(urls)} å€‹URL: {url}")
                result = await get_single_post_caption(url, context)
                results.append(result)
                
        finally:
            await context.close()
            await browser.close()
    
    return results

def save_batch_results_to_json(results: list, output_dir: str = "output") -> str:
    """ä¿å­˜æ‰¹é‡æŠ“å–çµæœç‚ºJSONæ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"instagram_batch_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    batch_data = {
        "batch_info": {
            "timestamp": datetime.datetime.now().isoformat(),
            "total_urls": len(results),
            "successful": sum(1 for r in results if r["success"]),
            "failed": sum(1 for r in results if not r["success"])
        },
        "results": results
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(batch_data, f, ensure_ascii=False, indent=2)
    
    return filepath

def validate_instagram_url(url: str) -> bool:
    """é©—è­‰Instagram URLæ ¼å¼"""
    try:
        parsed = urlparse(url)
        return (parsed.netloc in ['www.instagram.com', 'instagram.com'] and 
                ('/p/' in parsed.path or '/reel/' in parsed.path))
    except:
        return False

@app.route('/')
def index():
    """ä¸»é é¢"""
    return render_template('index.html')

@app.route('/extract', methods=['POST'])
def extract_captions():
    """è™•ç†æ–‡æ¡ˆæŠ“å–è«‹æ±‚"""
    try:
        data = request.get_json()
        urls_text = data.get('urls', '').strip()
        
        if not urls_text:
            return jsonify({"error": "è«‹æä¾›Instagram URL"}), 400
        
        # è§£æURLåˆ—è¡¨
        urls = [url.strip() for url in urls_text.split('\n') if url.strip()]
        
        # é©—è­‰URLæ ¼å¼
        valid_urls = []
        invalid_urls = []
        
        for url in urls:
            if validate_instagram_url(url):
                valid_urls.append(url)
            else:
                invalid_urls.append(url)
        
        if not valid_urls:
            return jsonify({"error": "æ²’æœ‰æœ‰æ•ˆçš„Instagram URL"}), 400
        
        if len(valid_urls) > 5:
            return jsonify({"error": "æœ€å¤šåªèƒ½è™•ç†5å€‹URL"}), 400
        
        # åŸ·è¡Œæ‰¹é‡æŠ“å–
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            results = loop.run_until_complete(batch_extract_captions(valid_urls))
        finally:
            loop.close()
        
        # ä¿å­˜çµæœ
        json_filepath = save_batch_results_to_json(results)
        
        # æº–å‚™å›æ‡‰
        response_data = {
            "success": True,
            "total_processed": len(results),
            "successful": sum(1 for r in results if r["success"]),
            "failed": sum(1 for r in results if not r["success"]),
            "invalid_urls": invalid_urls,
            "results": results,
            "json_file": json_filepath
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({"error": f"è™•ç†éŒ¯èª¤: {str(e)}"}), 500

@app.route('/health')
def health_check():
    """å¥åº·æª¢æŸ¥"""
    return jsonify({"status": "ok", "timestamp": datetime.datetime.now().isoformat()})

@app.route('/output/<filename>')
def download_file(filename):
    """ä¸‹è¼‰è¼¸å‡ºæ–‡ä»¶"""
    try:
        return send_file(
            os.path.join('output', filename),
            as_attachment=True,
            download_name=filename,
            mimetype='application/json'
        )
    except FileNotFoundError:
        return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404

if __name__ == '__main__':
    # ç¢ºä¿æ¨¡æ¿ç›®éŒ„å­˜åœ¨
    os.makedirs('templates', exist_ok=True)
    os.makedirs('output', exist_ok=True)
    
    print("ğŸš€ Instagram æ–‡æ¡ˆæŠ“å–å™¨ Web ç‰ˆæœ¬")
    print("ğŸ“ æ”¯æŒæ‰¹é‡æŠ“å–ï¼ˆæœ€å¤š5ç­†ï¼‰")
    print("ğŸ›¡ï¸ å…§å»ºååµæ¸¬ç­–ç•¥")
    print("ğŸŒ å•Ÿå‹•ä¸­...")
    
    app.run(debug=True, host='0.0.0.0', port=5000) 